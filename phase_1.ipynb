{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 - Data Acquisition and Refinement\n",
    "\n",
    "The goals for Phase 1, per the roadmap in the readme, are as follows:\n",
    "\n",
    "1. Obtain a dataset of news articles that includes the text content as well as a summary of each article. \n",
    "2. Explore the dataset to get a sense of the data, such as the number of articles, length of the articles and summaries, and distribution of topics and keywords.\n",
    "3. Clean and preprocess the data to remove unnecessary characters, punctuation, and stop words. \n",
    "4. Tokenize the text into words or subwords, and create input sequences and output summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 1: Obtain a Dataset\n",
    "\n",
    "We've obtained a dataset from Kaggle that contains 870,521 articles, each of which has a text content and a summary. The text content is the full article, and the summary is a short version of the article that summarizes the main points.\n",
    "\n",
    "For more information on the dataset, please see the dataset section in `README.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\devin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\devin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import WhitespaceTokenizer\n",
    "from nltk import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/news_summarization.csv') # expect this step to take about 30 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 2: Explore the Dataset\n",
    "\n",
    "We've explored the dataset and found the following information:\n",
    "- Total of 870,521 articles\n",
    "- Only 580,013 unique articles (about 1/3 of the articles are duplicates)\n",
    "- The most frequently occurring words are generic words like \"this\", \"that\", \"the\", \"a\", \"an\", etc.\n",
    "  - Should we remove these words from the dataset?\n",
    "\n",
    "The dataset contains the following columns:\n",
    "- `Unnamed: 0` - index, can be ignored\n",
    "- `ID` - unique ID for each article (appears to be a generated UUID)\n",
    "- `Content` - the text content of the article\n",
    "- `Summary` - the summary of the article\n",
    "- `Dataset` - the dataset that the article came from (XSum, CNN/Daily Mail, Multi-News)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Content</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f49ee725a0360aa6881ed1f7999cc531885dd06a</td>\n",
       "      <td>New York police are concerned drones could bec...</td>\n",
       "      <td>Police have investigated criminals who have ri...</td>\n",
       "      <td>CNN/Daily Mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>808fe317a53fbd3130c9b7563341a7eea6d15e94</td>\n",
       "      <td>By . Ryan Lipman . Perhaps Australian porn sta...</td>\n",
       "      <td>Porn star Angela White secretly filmed sex act...</td>\n",
       "      <td>CNN/Daily Mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98fd67bd343e58bc4e275bbb5a4ea454ec827c0d</td>\n",
       "      <td>This was, Sergio Garcia conceded, much like be...</td>\n",
       "      <td>American draws inspiration from fellow country...</td>\n",
       "      <td>CNN/Daily Mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e12b5bd7056287049d9ec98e41dbb287bd19a981</td>\n",
       "      <td>An Ebola outbreak that began in Guinea four mo...</td>\n",
       "      <td>World Health Organisation: 635 infections and ...</td>\n",
       "      <td>CNN/Daily Mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b83e8bcfcd51419849160e789b6658b21a9aedcd</td>\n",
       "      <td>By . Associated Press and Daily Mail Reporter ...</td>\n",
       "      <td>A sinkhole opened up at 5:15am this morning in...</td>\n",
       "      <td>CNN/Daily Mail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         ID  \\\n",
       "0  f49ee725a0360aa6881ed1f7999cc531885dd06a   \n",
       "1  808fe317a53fbd3130c9b7563341a7eea6d15e94   \n",
       "2  98fd67bd343e58bc4e275bbb5a4ea454ec827c0d   \n",
       "3  e12b5bd7056287049d9ec98e41dbb287bd19a981   \n",
       "4  b83e8bcfcd51419849160e789b6658b21a9aedcd   \n",
       "\n",
       "                                             Content  \\\n",
       "0  New York police are concerned drones could bec...   \n",
       "1  By . Ryan Lipman . Perhaps Australian porn sta...   \n",
       "2  This was, Sergio Garcia conceded, much like be...   \n",
       "3  An Ebola outbreak that began in Guinea four mo...   \n",
       "4  By . Associated Press and Daily Mail Reporter ...   \n",
       "\n",
       "                                             Summary         Dataset  \n",
       "0  Police have investigated criminals who have ri...  CNN/Daily Mail  \n",
       "1  Porn star Angela White secretly filmed sex act...  CNN/Daily Mail  \n",
       "2  American draws inspiration from fellow country...  CNN/Daily Mail  \n",
       "3  World Health Organisation: 635 infections and ...  CNN/Daily Mail  \n",
       "4  A sinkhole opened up at 5:15am this morning in...  CNN/Daily Mail  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(df.columns[0], axis=1) # unused index column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's find the average of the Content and Summary columns\n",
    "# \n",
    "# We can't analyze the entire dataset because it's too large, so we'll take a sample of 1000 rows.\n",
    "df_sample = df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mdf_sample\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlen\u001b[39m),\n\u001b[0;32m      3\u001b[0m     df_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSummary\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlen\u001b[39m),\n\u001b[0;32m      4\u001b[0m     df_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit())),\n\u001b[0;32m      5\u001b[0m     df_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSummary\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit())),\n\u001b[0;32m      6\u001b[0m     df_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m))),\n\u001b[0;32m      7\u001b[0m     df_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSummary\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m      8\u001b[0m ]\n\u001b[0;32m     10\u001b[0m sample \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent Length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSummary Length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent Words\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSummary Words\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent Sentences\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSummary Sentences\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdescribe()\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     12\u001b[0m sample\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_sample' is not defined"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    df_sample['Content'].apply(len),\n",
    "    df_sample['Summary'].apply(len),\n",
    "    df_sample['Content'].apply(lambda x: len(x.split())),\n",
    "    df_sample['Summary'].apply(lambda x: len(x.split())),\n",
    "    df_sample['Content'].apply(lambda x: len(x.split('.'))),\n",
    "    df_sample['Summary'].apply(lambda x: len(x.split('.')))\n",
    "]\n",
    "\n",
    "sample = pd.DataFrame(data, index=['Content Length', 'Summary Length', 'Content Words', 'Summary Words', 'Content Sentences', 'Summary Sentences']).T.describe().round(2)\n",
    "\n",
    "sample.loc['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m keywords \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(keywords)\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Let's plot the top 20 keywords as a word cloud.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# keywords = keywords[20:]\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m wordcloud \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mgenerate_from_frequencies(keywords)\n\u001b[0;32m     18\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m     19\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(wordcloud, interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Now let's look at distribution of topics and keywords.\n",
    "#\n",
    "# The dataset doesn't provide topics, so we'll parse keywords from the summary column.\n",
    "df_sample2 = df.sample(10000)\n",
    "df_sample2['Keywords'] = df_sample2['Summary'].apply(lambda x: x.split()[-5:])\n",
    "\n",
    "# Now let's list all the keywords and their frequencies.\n",
    "keywords = []\n",
    "for i in df_sample2['Keywords']:\n",
    "    keywords.extend(i)\n",
    "\n",
    "keywords = pd.Series(keywords).value_counts()\n",
    "\n",
    "# Let's plot the top 20 keywords as a word cloud.\n",
    "# keywords = keywords[20:]\n",
    "wordcloud = WordCloud(width=800, height=400, max_words=20, background_color='white').generate_from_frequencies(keywords)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 3: Clean and Preprocess the Data\n",
    "\n",
    "We'll clean and preprocess the data by removing articles that are duplicates or empty.\n",
    "\n",
    "I considered removing unnecessary punctuation and filler words, but I decided against it because I think it would be better to leave the data as-is and let the model learn how to handle these words. For example, if we remove the word \"the\", then the model will not be able to learn that \"the\" is a filler word and should be ignored.\n",
    "\n",
    "If the results are not good enough, then we can try removing unnecessary punctuation and filler words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['Content', 'Summary'])\n",
    "df = df.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 4: Tokenize the Text\n",
    "\n",
    "We'll tokenize the text by splitting the text into words and creating input sequences and output summaries. We'll also create a dictionary that maps each word to a unique integer. We'll use this dictionary to convert the words in the input sequences and output summaries to integers.\n",
    "\n",
    "We'll also create a reverse dictionary that maps each integer back to the original word. We'll use this reverse dictionary to convert the integers back to words.\n",
    "\n",
    "We'll also create a dictionary that maps each word to the number of times it appears in the dataset. We'll use this dictionary to filter out words that appear less than a certain number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [New, York, police, are, concerned, drone, cou...\n",
       "1         [By, ., Ryan, Lipman, ., Perhaps, Australian, ...\n",
       "2         [This, was,, Sergio, Garcia, conceded,, much, ...\n",
       "3         [An, Ebola, outbreak, that, began, in, Guinea,...\n",
       "4         [By, ., Associated, Press, and, Daily, Mail, R...\n",
       "                                ...                        \n",
       "870508    [The, deal, will, begin, in, January, and, mea...\n",
       "870512    [Rabbis, Mendel, Epstein, and, Martin, Wolmark...\n",
       "870513    [The, Nigeria, Football, Federation, (NFF), wa...\n",
       "870514    [His, discouraging, start,, which, began, with...\n",
       "870515    [The, first, study, of, it, kind, in, NI, char...\n",
       "Name: Content_Tokens, Length: 523948, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk = WhitespaceTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df['Summary_Tokens'] = df['Summary'].apply(lambda x: tk.tokenize(x))\n",
    "df['Content_Tokens'] = df['Content'].apply(lambda x: tk.tokenize(x))\n",
    "\n",
    "def lemmatize_ser(ser):\n",
    "    for i_l, l in enumerate(ser):\n",
    "        for i_w, w in enumerate(l):\n",
    "            ser.iloc[i_l][i_w]= lemmatizer.lemmatize(w)\n",
    "            \n",
    "lemmatize_ser(df['Summary_Tokens'])\n",
    "lemmatize_ser(df['Content_Tokens'])\n",
    "\n",
    "# display lemmatized version of tokens\n",
    "df['Content_Tokens']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4286f1ee66e41fbf26cca8876906444877a06270d30416029ae0afaadf35139a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
